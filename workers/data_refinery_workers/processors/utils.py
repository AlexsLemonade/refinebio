import os
import random
import string
import subprocess
import yaml

from enum import Enum, unique
from typing import List, Dict, Callable
from django.utils import timezone
from data_refinery_common.models import (
    ProcessorJob,
    Pipeline,
    Processor,
    Sample,
    OriginalFile,
    Dataset,
    ProcessorJobOriginalFileAssociation,
    ProcessorJobDatasetAssociation,
    OriginalFileSampleAssociation
)
from data_refinery_workers._version import __version__
from data_refinery_common.logging import get_and_configure_logger
from data_refinery_common.utils import get_worker_id, get_env_variable

logger = get_and_configure_logger(__name__)
S3_BUCKET_NAME = get_env_variable("S3_BUCKET_NAME", "data-refinery")
DIRNAME = os.path.dirname(os.path.abs_path(__file__))
ZENODO_URL_FILENAME = "/etc/id_refinery_url.txt"  # Generated by Dockerfile.no_op

def start_job(job_context: Dict):
    """A processor function to start jobs.

    Record in the database that this job is being started and
    retrieves the job's batches from the database and adds them to the
    dictionary passed in with the key 'batches'.
    """
    job = job_context["job"]
    job.worker_id = get_worker_id()
    job.worker_version = __version__
    job.start_time = timezone.now()
    job.save()

    logger.info("Starting processor Job.", processor_job=job.id, pipeline=job.pipeline_applied)

    # The Smasher is the only job type which doesn't take OriginalFiles,
    # so we make an exception here.
    if job.pipeline_applied != "SMASHER":
        relations = ProcessorJobOriginalFileAssociation.objects.filter(processor_job=job)
        original_files = OriginalFile.objects.filter(id__in=relations.values('original_file_id'))

        if len(original_files) == 0:
            logger.error("No files found.", processor_job=job.id)
            job_context["success"] = False
            return job_context

        job_context["original_files"] = original_files
        original_file = job_context['original_files'][0]
        assocs = OriginalFileSampleAssociation.objects.filter(original_file=original_file)
        samples = Sample.objects.filter(id__in=assocs.values('sample_id')).distinct()
        job_context['samples'] = samples
        job_context["computed_files"] = []

    else:
        relations = ProcessorJobDatasetAssociation.objects.filter(processor_job=job)

        # This should never be more than one!
        dataset = Dataset.objects.filter(id__in=relations.values('dataset_id')).first()
        dataset.is_processing = True
        dataset.save()

        # Get the samples to smash
        job_context["dataset"] = dataset
        job_context["samples"] = dataset.get_aggregated_samples()
        job_context["experiments"] = dataset.get_experiments()

        # Just in case
        job_context["original_files"] = []
        job_context["computed_files"] = []

    return job_context


def end_job(job_context: Dict, abort=False):
    """A processor function to end jobs.

    Record in the database that this job has completed and that
    the samples have been processed if not aborted.
    """
    job = job_context["job"]

    if "success" in job_context:
        success = job_context["success"]
    else:
        success = True

    if not abort:
        if job_context.get("success", False) and job_context["job"].pipeline_applied != "SMASHER":
            # This handles most of our cases
            for sample in job_context["samples"]:
                sample.is_processed = True
                sample.save()

            # Explicitly for the single-salmon scenario
            if 'sample' in job_context:
                sample = job_context['sample']
                sample.is_processed = True
                sample.save()

    # S3-sync Original Files
    for original_files in job_context['original_files']:
        # Ensure even distribution across S3 servers
        nonce = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))
        result = original_files.sync_to_s3(S3_BUCKET_NAME, nonce + "_" + original_files.filename)
        if result:
            original_files.delete_local_file()

    # S3-sync Computed Files
    for computed_file in job_context['computed_files']:
        # Ensure even distribution across S3 servers
        nonce = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(8))
        result = computed_file.sync_to_s3(S3_BUCKET_NAME, nonce + "_" + computed_file.filename)
        if result:
            computed_file.delete_local_file()

        # If the sample-level pipeline includes any steps, save it.
        pipeline = job_context['pipeline']
        if len(pipeline.steps):
            pipeline.save()

    job.success = success
    job.end_time = timezone.now()
    job.save()

    if success:
        logger.info("Processor job completed successfully.", processor_job=job.id)
    else:
        logger.info("Processor job failed!", processor_job=job.id)

    # Return Final Job context so testers can check it
    return job_context


def upload_processed_files(job_context: Dict) -> Dict:
    """Uploads the processed files and removes the temp dir for the job.

    If job_context contains a "files_to_upload" key then only those
    files will be uploaded. Otherwise all files will be uploaded.
    If job_context contains a "job_dir_prefix" key then that will be
    passed through to the file methods as the `dir_name` parameter.
    """
    if "files_to_upload" in job_context:
        files = job_context["files_to_upload"]
    else:
        files = File.objects.filter(batch__in=job_context["batches"])

    if "job_dir_prefix" in job_context:
        job_dir_prefix = job_context["job_dir_prefix"]
    else:
        job_dir_prefix = None

    try:
        for file in files:
            file.upload_processed_file(job_dir_prefix)
    except Exception:
        logger.exception("Exception caught while uploading processed file %s",
                         batch=files[0].batch.id,
                         processor_job=job_context["job_id"])
        job_context["job"].failure_reason = "Exception caught while uploading processed file."
        job_context["success"] = False
        return job_context
    finally:
        # Whether or not uploading was successful, the job is over so
        # clean up the temp directory.
        files[0].remove_temp_directory(job_dir_prefix)

    return job_context


def cleanup_raw_files(job_context: Dict) -> Dict:
    """Tries to clean up raw files for the job.

    If we fail to remove the raw files, the job is still done enough
    to call a success, therefore we don't mark it as a failure.
    However logging will be important so the problem can be
    identified and the raw files cleaned up.
    """
    files = File.objects.filter(batch__in=job_context["batches"])
    for file in files:
        try:
            file.remove_raw_files()
        except:
            # If we fail to remove the raw files, the job is still done
            # enough to call a success. However logging will be important
            # so the problem can be identified and the raw files cleaned up.
            logger.exception("Exception caught while removing raw files %s",
                             file.get_temp_pre_path(),
                             batch=file.batch.id,
                             processor_job=job_context["job_id"])

    return job_context


def run_pipeline(start_value: Dict, pipeline: List[Callable]):
    """Runs a pipeline of processor functions.

    start_value must contain a key 'job_id' which is a valid id for a
    ProcessorJob record.

    Each processor fuction must accept a dictionary and return a
    dictionary.

    Any processor function which returns a dictionary containing a key
    of 'success' with a value of False will cause the pipeline to
    terminate with a call to utils.end_job.

    The key 'job' is reserved for the ProcessorJob currently being
    run.  The key 'batches' is reserved for the Batches that are
    currently being processed.  It is required that the dictionary
    returned by each processor function preserve the mappings for
    'job' and 'batches' that were passed into it.
    """
    job_id = start_value["job_id"]
    try:
        job = ProcessorJob.objects.get(id=job_id)
    except ProcessorJob.DoesNotExist:
        logger.error("Cannot find processor job record.", processor_job=job_id)
        return

    if len(pipeline) == 0:
        logger.error("Empty pipeline specified.",
                     procesor_job=job_id)

    last_result = start_value
    last_result["job"] = job
    for processor in pipeline:
        try:
            last_result = processor(last_result)
        except Exception:
            logger.exception("Unhandled exception caught while running processor function %s in pipeline",
                             processor.__name__,
                             processor_job=job_id)
            last_result["success"] = False
            return end_job(last_result)

        if "success" in last_result and last_result["success"] is False:
            logger.error("Processor function %s failed. Terminating pipeline.",
                         processor.__name__,
                         processor_job=job_id,
                         failure_reason=last_result["job"].failure_reason)
            return end_job(last_result)

        if last_result.get("abort", False):
            return end_job(last_result, abort=True)

    return last_result


@unique
class PipelineEnum(Enum):
    """Hardcoded pipeline names."""

    AGILENT_TWOCOLOR = "Agilent Two Color"
    ARRAY_EXPRESS = "Array Express"
    ILLUMINA = "Illumina"
    NO_OP = "No Op"
    SALMON = "Salmon"
    SMASHER = "Smasher"
    TX_INDEX = "Transcriptome Index"


@unique
class ProcessorEnum(Enum):
    """Hardcoded processor info in each pipeline."""

    # One processor in "Agilent Two Color" pipeline
    AGILENT_TWOCOLOR = {
        "name": "Agilent SCAN TwoColor",
        "docker_img": "not_available_yet",
        "yml_file": "agilent_twocolor.yml"
    }

    # One processor in "Array Express" pipeline
    AFFYMETRIX_SCAN = {
        "name": "Affymetrix SCAN",
        "docker_img": "dr_affymetrix",
        "yml_file": "affymetrix.yml"
    }

    # One processor in "Illumina" pipeline
    ILLUMINA_SCAN = {
        "name": "Illumina SCAN",
        "docker_img": "dr_illumina",
        "yml_file": "illumina.yml"
    }

    # One processor in "No Op" pipeline
    SUBMITTER_PROCESSED = {
        "name": "Submitter-processed",
        "docker_img": "dr_no_op",
        "yml_file": "no_op.yml"
    }

    # Four processors in "Salmon" pipeline
    MULTIQC = {
        "name": "MultiQC",
        "docker_img": "dr_salmon",
        "yml_file": "multiqc.yml"
    }
    SALMON_QUANT = {
        "name": "Salmon Quant",
        "docker_img": "dr_salmon",
        "yml_file": "salmon_quant.yml"
    }
    SALMONTOOLS = {
        "name": "Salmontools",
        "docker_img": "dr_salmon",
        "yml_file": "salmontools.yml"
    }
    TXIMPORT = {
        "name": "Tximport",
        "docker_img": "dr_salmon",
        "yml_file": "tximport.yml"
    }

    # One processor in "Smasher" pipeline
    SMASHER = {
        "name": "Smasher",
        "docker_img": "dr_smasher",
        "yml_file": "smasher.yml"
    }

    # One processor in "Transcriptome Index" pipeline
    TX_INDEX = {
        "name": "Transcriptome Index",
        "docker_img": "dr_transcriptome",
        "yml_file": "transcriptome_index.yml"
    }

    @classmethod
    def has_key(cls, key):
        """Class method that tells whether a certain key exists."""
        return key in cls.__members__


def createTestProcessors():
    """Creates dummy processors for all unit test cases.
    (This function should be called ONLY by test modules).
    """

    for label in ProcessorEnum:
        Processor.objects.create(name=label.value, version=__version__)


def get_os_distro():
    """Returns a string of OS distribution.
    Since we are using Docker, this function only considers Linux distribution.
    Other alternatives on Linux: /etc/os-release, /etc/lsb-release
    As a matter of fact, "/etc/issue" doesn't exist on Mac OS X.  We can use
    "sw_vers" command to find its OS information.
    A more cross-platform solution is using "platform" module.
    """

    with open('/etc/issue') as distro_fh:
      return distro_fh.readline().strip('\l\n\\n ')


def get_os_pkgs(pkg_list):
    """Returns a dictionary whose key is the name of a os-lvel package
    and value is the package's version. This function assumes the package
    manager is Debian-based (dpkg/apt). It can be a nightmaire to support
    all different package managers on Linux.
    """

    pkg_info = []  # Use "list" to keep the order in YAML file.
    for pkg in pkg_list:
        process_done = subprocess.run(['dpkg-query', '--show', pkg],
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
        if process_done.returncode:
            raise Exception("OS-level package %s not found: %s" %
                            (pkg, process_done.stderr.decode().strip())
            )

        version = process_done.stdout.decode().strip().split('\t')[-1]
        pkg_info.append({pkg: version})

    return pkg_info


def get_cmd_lines(cmd_list):
    """Returns a dictionary whose key is the name of a command line and
    value is the command's version.  The version is always retrieved by
    "<cmd --version" command.
    """

    cmd_info = []  # Use "list" to keep the order in YAML file.
    for cmd in cmd_list:
        args = [cmd, '--version']

        # As of 08/01/2018, "--version" or "-v" option is NOT supported by
        # "rsem-prepare-reference" command, but it is supported by
        # "rsem-calculate-expression", which is another command in RSEM pkg.
        if cmd == "rsem-prepare-reference":
            args[0] = 'rsem-calculate-expression'

        process_done = subprocess.run(args,
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
        if process_done.returncode:
            raise Exception("Command line %s version check failed: %s" %
                            (cmd, process_done.stderr.decode().strip())
            )

        output_bytes = process_done.stdout
        # This is a workaround for "salmon --version" and "salmontools --version"
        # commands, whose outputs are sent to stderr instead of stdout.
        if not len(output_bytes):
            output_bytes = process_done.stderr

        version = output_bytes.decode().strip().split()[-1]
        cmd_info.append({cmd: version})

    return cmd_info


def get_pip_pkgs(pkg_list):
    """Returns a dictionary whose key is the name of a pip-installed package
    and value is the package's version.  Instead of using a command like:
      `pip show pkg | grep Version | awk '{print $2}'`
    to get version for each package, we save the output of `pip freeze` as a
    dictionary first, then check the version of packages in pkg_list.
    This approach launches the subprocess only once and (hopefully) saves some
    computational resource.
    """

    process_done = subprocess.run(['pip', 'freeze'],
                                  stdout=subprocess.PIPE,
                                  stderr=subprocess.PIPE)
    if process_done.returncode:
        riase Exception("'pip freeze' failed: %s" % process_done.stderr.decode().strip())

    frozen_pkgs = dict()
    for item in process_done.stdout.decode().split():
        name, version = item.split("==")
        frozen_pkgs[name] = version

    pkg_info = []  # Use "list" to keep the order in YAML file.
    for pkg in pkg_list:
        try:
            version = frozen_pkgs[pkg]
        except KeyError:
            raise Exception("Pip package not found: %s" % pkg)

        pkg_info.append({pkg: version})

    return pkg_info


def get_bioc_version():
    """This function returns a string that is the version of "Bioconductor"
    package in R.  Note that "Bioconductor" is special in that its package
    info is NOT included in the data frame returned by installed.packages().
    """

    r_command = "tools:::.BioC_version_associated_with_R_version()"
    process_done = subprocess.run(['Rscript', '-e', r_command],
                                  stdout=subprocess.PIPE,
                                  stderr=subprocess.PIPE)
    if process_done.returncode:
        raise Exception('R command failed to retrieve Bioconductor version: %s' %
                        process_done.stderr.decode().strip()
        )

    version = process_done.stdout.decode().strip().split()[-1]
    version = version[1:-1]  # Remove the leading and trailing non-ascii characters.

    if len(version) == 0:
        raise Exception('Bioconductor not found')

    return version


def get_r_pkgs(pkg_list):
    """Returns a dictionary whose key is the name of a R package
    and value is the package's version.
    """

    # Use "Rscript -e <R_commands>" command to get all user-installed R packages.
    r_commands = "packages.df <- as.data.frame(installed.packages()[, c(1, 3:4)]); \
    packages.df <- packages.df[is.na(packages.df$Priority), 1:2, drop=FALSE]; \
    colnames(packages.df) <- NULL; \
    print(packages.df, row.names=FALSE);"

    process_done = subprocess.run(['Rscript', '-e', r_commands],
                                  stdout=subprocess.PIPE,
                                  stderr=subprocess.PIPE)
    if process_done.returncode:
        raise Exception('R command failed to retrieves installed packages: %s' %
                        process_done.stderr.decode().strip()
        )

    r_pkgs = dict()
    for item in process_done.stdout.decode().strip().split('\n'):
        name, version = item.strip().split()
        r_pkgs[name] = version

    # "Brainarray" is a collection that consists of 121 ".*ensgprobe" packages.
    # They share the same version number, so we use 'hgu133plus2hsensgprobe'
    # package to report this uniform version.
    ba_proxy_pkg = 'hgu133plus2hsensgprobe'

    pkg_info = []  # Use "list" to keep the order in YAML file.
    for pkg in pkg_list:
        if pkg == 'Bioconductor':
            version = get_bioc_version()
        else:
            try:
                version = r_pkgs[pkg] if pkg != "Brainarray" else r_pkgs[ba_proxy_pkg]
            except KeyError:
                raise Exception("R package not found: %s" % pkg)

        pkg_info.append({pkg: version})

    return pkg_info


def get_checksums(pkg_list):
    """Returns checksums of files in file_list."""

    checksums = []
    for pkg in pkg_list:
        abs_filepath = os.path.join(DIRNAME, pkg)
        process_done = subprocess.run(['md5sum', abs_filepath],
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
        if process_done.returncode:
            raise Exception("md5sum command error:",
                            process_done.stderr.decode().strip())
        version = process_done.stdout.decode().strip().split()[0]
        checksums.append({pkg: version})

    return checksums

def get_zenodo_url():
    """Returns zeno_url from ZENODO_URL_FILENAME, which is a one-liner."""

    with open(ZENODO_URL_FILENAME) as zenodo_fh:
        return zenodo_fh.readline().strip()


def get_others(pkg_list):
    """Find version information of any other stuff."""

    pkg_info = []  # Use "list" to keep the order in YAML file.
    for pkg in pkg_list:
        if pkg == 'zenodo_url':
            version = get_zenodo_url()
        else:
            raise Exception("Unknown variable: %s" % pkg)

        pkg_info.append({pkg: version})

    return pkg_info

def get_runtime_env(yml_filename):
    """Reads input YAML file and returns a dictionary that includes
    package version information.
    """

    # To keep the order of categories in YAML file, we are using "list"
    # instead of "dict" because insertion order in "dict" type is not
    # official until Python 3.6, and JSONField in Django does not support
    # collections.OrderedDict.
    version_info = []
    with open(yml_filename) as yml_fh:
        pkgs = yaml.load(yml_fh)
        for pkg_type, pkg_list in pkgs.items():
            if pkg_type == 'os_distribution':
                value = get_os_distro()
            elif pkg_type == 'os_pkg':
                value = get_os_pkgs(pkg_list)
            elif pkg_type == 'cmd_line':
                value = get_cmd_lines(pkg_list)
            elif pkg_type == 'python':
                value = get_pip_pkgs(pkg_list)
            elif pkg_type == 'R':
                value = get_r_pkgs(pkg_list)
            elif pkg_type == 'checksum':
                values = get_checksums(pkg_list)
            elif pkg_type == 'other':
                value = get_others(pkg_list)
            else:
                raise Exception("Unknown category in %s: %s" % (yml_filename, pkg_type))

            version_info.append({pkg_type: value})

    return version_info


def find_processor(enum_key):
    """Retursn either a newly created Processor record, or the one in
    database that matches the current processor name, version and environment.
    """

    name = ProcessorEnum[enum_key].value.name
    docker_image = ProcessorEnum[enum_key].value.docker_img

    yml_path = os.path.join(DIRNAME, ProcessorEnum[enum_key].value.yml_file)
    # In current implementation, ALWAYS get the runtime environment.
    environment = get_runtime_env(yml_path)

    return Processor.objects.get_or_create(name=name,
                                           version=__version__,
                                           docker_image=docker_image,
                                           environment=environment)
